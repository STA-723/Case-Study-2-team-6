---
title: "JW-Workspace"
author: "Justin Weltz"
date: "1/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(dplyr)
require(ggplot2)
```



```{r}
require(readr)
require(lubridate)
data <- read_csv("AB_NYC_2019.csv")
data <- data %>% 
  mutate(last_review = ymd(last_review)) %>% 
  mutate(months_inactive = round(as.numeric(ymd("2019-07-08") - last_review)/30.42)) %>% 
  mutate(months_active = (number_of_reviews - reviews_per_month*months_inactive)/reviews_per_month) %>% 
  mutate(months_active = case_when(
    months_active >=1 ~ months_active, 
    months_active <1 ~ 1
  )) %>% 
  mutate(nrml_avg_reviews = case_when(
    reviews_per_month>0 ~ number_of_reviews/months_active,
    is.na(reviews_per_month) ~ 0))
data %>% 
  filter(number_of_reviews == 30) %>% 
  arrange(last_review, months_active) %>% 
  dplyr::select(number_of_reviews, last_review, months_active, reviews_per_month, nrml_avg_reviews)
```


```{r}

summary(data)

data %>% 
  arrange(desc(price)) %>% 
  select(price) 

data %>% 
  group_by(neighbourhood_group) %>% 
  summarise(avg_price = mean(price))
data %>% 
  group_by(neighbourhood_group, neighbourhood) %>% 
  summarise(avg_price = mean(price))


data %>% 
  filter(!is.na(reviews_per_month)) %>% 
  group_by(neighbourhood_group, neighbourhood) %>% 
  summarise(avg_reviews_per_month = mean(reviews_per_month))
data %>% 
  group_by(neighbourhood_group, neighbourhood) %>% 
  count() %>% 
  arrange (n)
data %>% 
  arrange(desc(price)) %>% 
  slice(1:2445) %>% 
  select(price)

data %>% filter(price < 1000) %>% ggplot(aes(price,reviews_per_month, color = neighbourhood_group)) + geom_smooth()

data %>% filter(price < 1000) %>% ggplot(aes(price,reviews_per_month, color = room_type )) + geom_smooth()

data %>% filter(price < 1000) %>% ggplot(aes(price,number_of_reviews, color = neighbourhood_group)) + geom_smooth()

data %>% filter(price < 1000) %>% ggplot(aes(price,number_of_reviews, color = room_type)) + geom_smooth()

data %>% filter(number_of_reviews < 100 & number_of_reviews>0) %>% ggplot(aes(number_of_reviews)) + geom_histogram()

data %>% filter(availability_365 < 10) %>% ggplot(aes(availability_365)) + geom_histogram()

data %>% filter(availability_365 == 0 & number_of_reviews != 0)

data %>% filter(availability_365 == 0)

data %>% filter(number_of_reviews != 0)

data %>% filter(number_of_reviews == 0) %>% filter(price < 500) %>% ggplot(aes(price)) + geom_histogram()

data %>% filter(number_of_reviews != 0) %>%  filter(price < 500) %>% ggplot(aes(price)) + geom_histogram()

data %>% filter(number_of_reviews == 0) %>% ggplot(aes(neighbourhood_group)) + geom_bar()

data %>% filter(number_of_reviews > 0) %>% ggplot(aes(neighbourhood_group)) + geom_bar()

data %>% filter(availability_365 == 0) %>% ggplot(aes(neighbourhood_group)) + geom_bar()

data %>% filter(availability_365 > 0) %>% ggplot(aes(neighbourhood_group)) + geom_bar()

data %>% filter(availability_365 == 0) %>% ggplot(aes(neighbourhood_group)) + geom_bar()

data %>% filter(availability_365 > 0) %>% ggplot(aes(neighbourhood_group)) + geom_bar()

data %>% filter(availability_365 == 0) %>% ggplot(aes(room_type)) + geom_bar()

data %>% filter(availability_365 > 0) %>% ggplot(aes(room_type)) + geom_bar()


data %>% filter(number_of_reviews == 0) %>% ggplot(aes(room_type)) + geom_bar()

data %>% filter(number_of_reviews > 0) %>% ggplot(aes(room_type)) + geom_bar()

```


Geographic Data

```{r}
#install.packages("sf")
#install.packages("mapview")
library(sf)
library(mapview)
#data
#locations_sf <- st_as_sf(data,c("latitude", "longitude"), crs = 4326)


data %>% ggplot(aes(x=latitude, y=longitude, size= number_of_reviews, color = neighbourhood_group)) + geom_point

data %>% filter(neighbourhood_group == "Queens") %>% ggplot(aes(x=latitude, y=longitude, size= number_of_reviews, )) + geom_point()

sum(data$minimum_nights>365)
```

Clustering:

```{r}
#install.packages("StatMatch")
#install.packages("cluster")
require(StatMatch)
require(cluster)
data_clust <- data %>% dplyr:: select(latitude, longitude, price, neighbourhood_group, neighbourhood)
data_clust <- data_clust %>% mutate(neighbourhood_group = as.factor(neighbourhood_group)) %>% mutate(neighbourhood = as.factor(neighbourhood)) %>% filter(price < 100) %>% filter(neighbourhood_group == "Manhattan")

data_clust <- data_clust[c(1:100, 200:600, 700:1000),]


data_clust


gower_dist <- daisy(data_clust, metric = "gower")
summary(gower_dist)

gower_mat <- as.matrix(gower_dist)

pam_fit <- pam(gower_dist, diss = TRUE, k=10)

#pam_fit$clustering

clustered <- cbind(data_clust, pam_fit$clustering)

clustered$`pam_fit$clustering`<- as.factor(clustered$`pam_fit$clustering`)

clustered %>% ggplot(aes(latitude, longitude, color = `pam_fit$clustering`)) + geom_point()
```


```{r}
#install.packages("spdep")
require(sp)
require(spdep)
points <- SpatialPoints(cbind(data_clust$latitude, data_clust$longitude))
Sy8_nb <- knearneigh(points, longlat = T, k = 1)

Sy8_nb <- knn2nb(Sy8_nb)

Sy8_nb

dsts <- unlist(nbdists(Sy8_nb, points))
min <- max(dsts)


dup <- which(duplicated(data_clust %>% dplyr::select(longitude, latitude)))

if(length(dup > 0)){
data_clust <- data_clust[-dup, ]
}


latlon <- data_clust %>% dplyr::select(longitude, latitude)

euclidean_dist <- daisy(latlon, metric = "euclidean")

euclidean_mat <- as.matrix(euclidean_dist)
dim(as.matrix(euclidean_dist))


adjacency <- matrix(0, ncol(euclidean_mat), ncol(euclidean_mat))
for (i in 1:ncol(euclidean_mat)){
  for (j in 1:ncol(euclidean_mat)){
    if (euclidean_mat[i,j] < min ){
      adjacency[i,j] = 1
    }
  }
}

```

```{r}

#install.packages("CARBayes")
require(CARBayes)

new_data_clust <- data %>% dplyr::select(latitude, longitude, price, neighbourhood_group, neighbourhood, number_of_reviews) %>% mutate(neighbourhood_group = as.factor(neighbourhood_group)) %>% mutate(neighbourhood = as.factor(neighbourhood)) %>% filter(price < 100) %>% filter(neighbourhood_group == "Manhattan")

new_data_clust <- new_data_clust[c(1:100, 200:600, 700:1000),]

#View(new_data_clust %>% group_by(neighbourhood) %>% summarise(n = n()))

dup <- which(duplicated(new_data_clust %>% dplyr::select(longitude, latitude)))

if(length(dup > 0)){
data_clust <- data_clust[-dup, ]
}

summary(lm(number_of_reviews ~ neighbourhood, data = new_data_clust))

mod<- lm(number_of_reviews ~ neighbourhood, data = new_data_clust)

model_data <- model.matrix(mod)

model_data <- cbind(new_data_clust$number_of_reviews, model_data)

model_data <- data.frame(model_data)

model_data

#View(model_data[,-2])

model_1 <- S.CARleroux(data = model_data[,-2], formula = V1 ~ ., family="gaussian", W= adjacency, burnin=1000, n.sample=100000, thin=20)



model_2 <- S.glm(data = model_data[,-2], formula = V1 ~ ., family="gaussian", burnin=1000, n.sample=100000, thin=20)

coef(model_1) - coef(model_2)

```

Let's try an analysis of lower manhattan:

```{r}
set.seed(2)

man_data_clust <- data %>% dplyr::select(latitude, longitude, price, neighbourhood_group, neighbourhood, nrml_avg_reviews, minimum_nights) %>% mutate(neighbourhood_group = as.factor(neighbourhood_group)) %>% mutate(neighbourhood = as.factor(neighbourhood))  %>% filter(minimum_nights < 30) %>% filter(neighbourhood_group == "Manhattan")

man_data_clust %>% ggplot(aes(latitude, longitude, color= neighbourhood)) + geom_point()

lowman_data_clust <- man_data_clust 

lowman_data_clust %>% ggplot(aes(latitude, longitude, color= neighbourhood)) + geom_point()

samp_lowman_data <- sample_n(lowman_data_clust,1000)

samp_lowman_data %>% ggplot(aes(latitude, longitude, color= neighbourhood)) + geom_point()


```

Make the Adjacency Matrix
```{r}

data_clust <- samp_lowman_data
  
points <- SpatialPoints(cbind(data_clust$latitude, data_clust$longitude))
Sy8_nb <- knearneigh(points, longlat = T, k = 1)

Sy8_nb <- knn2nb(Sy8_nb)

Sy8_nb

dsts <- unlist(nbdists(Sy8_nb, points))
min <- max(dsts)


dup <- which(duplicated(data_clust %>% dplyr::select(longitude, latitude)))

if(length(dup > 0)){
data_clust <- data_clust[-dup, ]
}

latlon <- data_clust %>% dplyr::select(longitude, latitude)

euclidean_dist <- daisy(latlon, metric = "euclidean")

euclidean_mat <- as.matrix(euclidean_dist)
dim(as.matrix(euclidean_dist))


adjacency <- matrix(0, ncol(euclidean_mat), ncol(euclidean_mat))
for (i in 1:ncol(euclidean_mat)){
  for (j in 1:ncol(euclidean_mat)){
    if (euclidean_mat[i,j] < min ){
      adjacency[i,j] = 1
    }
  }
}
mean(rowSums(adjacency))
lowman_data_sample <- data_clust
```


Run the Model

```{r}

#install.packages("CARBayes")
require(CARBayes)

new_data_clust <- lowman_data_sample

summary(lm(nrml_avg_reviews~ neighbourhood, data = new_data_clust))

mod<- lm(nrml_avg_reviews ~ neighbourhood, data = new_data_clust)

model_data <- model.matrix(mod)

#model_data

model_data <- cbind(new_data_clust$nrml_avg_reviews, model_data)

model_data <- data.frame(model_data)

model_data

#View(model_data[,-2])

memory.limit(size = 20000)

memory.limit()

model_data

model_1 <- S.CARleroux(data = model_data[-2], formula = V1 ~ ., family="gaussian", W= adjacency, burnin=100, n.sample=100000, thin=10)


model_2 <- S.glm(data =  model_data[-2], formula = V1 ~ ., family="gaussian", burnin=1000, n.sample=100000, thin=10)

summary(lm(data= new_data_clust, nrml_avg_reviews ~ neighbourhood))


print(model_1)
 print(model_2)
coef(model_1) - coef(model_2)


```




Let's try an analysis of the Bronx:

```{r}
set.seed(2)

man_data_clust <- data %>% dplyr::select(latitude, longitude, price, neighbourhood_group, neighbourhood, nrml_avg_reviews, minimum_nights) %>% mutate(neighbourhood_group = as.factor(neighbourhood_group)) %>% mutate(neighbourhood = as.factor(neighbourhood))  %>% filter(minimum_nights < 30) %>% filter(neighbourhood_group == "Bronx")

man_data_clust %>% ggplot(aes(latitude, longitude, color= neighbourhood)) + geom_point()

lowman_data_clust <- man_data_clust 

#lowman_data_clust %>% ggplot(aes(latitude, longitude, color= neighbourhood)) + geom_point()

samp_lowman_data <- sample_n(lowman_data_clust,1000)

samp_lowman_data %>% ggplot(aes(latitude, longitude, color= neighbourhood)) + geom_point()


```

Make the Adjacency Matrix
```{r}

data_clust <- samp_lowman_data
  
points <- SpatialPoints(cbind(data_clust$latitude, data_clust$longitude))
Sy8_nb <- knearneigh(points, longlat = T, k = 1)

Sy8_nb <- knn2nb(Sy8_nb)

Sy8_nb

dsts <- unlist(nbdists(Sy8_nb, points))
min <- max(dsts)


dup <- which(duplicated(data_clust %>% dplyr::select(longitude, latitude)))

if(length(dup > 0)){
data_clust <- data_clust[-dup, ]
}

latlon <- data_clust %>% dplyr::select(longitude, latitude)

euclidean_dist <- daisy(latlon, metric = "euclidean")

euclidean_mat <- as.matrix(euclidean_dist)
dim(as.matrix(euclidean_dist))


adjacency <- matrix(0, ncol(euclidean_mat), ncol(euclidean_mat))
for (i in 1:ncol(euclidean_mat)){
  for (j in 1:ncol(euclidean_mat)){
    if (euclidean_mat[i,j] < min ){
      adjacency[i,j] = 1
    }
  }
}
mean(rowSums(adjacency))
lowman_data_sample <- data_clust
```


Run the Model

```{r}

#install.packages("CARBayes")
require(CARBayes)

new_data_clust <- lowman_data_sample

summary(lm(nrml_avg_reviews~ neighbourhood, data = new_data_clust))

mod<- lm(nrml_avg_reviews ~ neighbourhood, data = new_data_clust)

model_data <- model.matrix(mod)

#model_data

model_data <- cbind(new_data_clust$nrml_avg_reviews, model_data)

model_data <- data.frame(model_data)

model_data

#View(model_data[,-2])

memory.limit(size = 20000)

memory.limit()

model_data

model_1 <- S.CARleroux(data = model_data[-2], formula = V1 ~ ., family="gaussian", W= adjacency, burnin=100, n.sample=100000, thin=10)


model_2 <- S.glm(data =  model_data[-2], formula = V1 ~ ., family="gaussian", burnin=1000, n.sample=100000, thin=10)

summary(lm(data= new_data_clust, nrml_avg_reviews ~ neighbourhood))


print(model_1)
 print(model_2)
coef(model_1) - coef(model_2)


```

Longitude and Latitude - clustering - nyc package? - latent SES variable

calcium process

conditionally autoregressive models, markov random field 





Textual Analysis




Zeros - Reviews - approaches


covariates

piece-wise linear

change-point 



Shrinkage - extreme differences in observations - defining prior

Approaches to reviews -positive or negative - threshold




profitable? Price data?




Name data - classify somehow



Last Review data - time series distribution

Outlier analysis using BMA

Seperate analysis for availiability 

House-type

Alan Gelfan - point-referenced spatio-temporal



Down-weight non-active because not reflective of current trends

censoring