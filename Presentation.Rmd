---
title: "Popularity of NYC Airbnb Listings"
author: "Olivier Binette, Justin Weltz, and Brian Kundinger"
output: 
  beamer_presentation
---
```{r, echo=FALSE, warning=FALSE, fig.height=5, include = FALSE}
chooseCRANmirror(graphics=FALSE, ind=1)
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(ggplot2)
library(quanteda)
library(tidytext)
library(lme4)
library(car)
library(lme4)
library(jtools)
library(xtable)
library(MASS)
source("prettyplot.R")
```

## Goals

- Determine "neighborhood brand" effects.

- Examine the features of a popular listing.

## Data cleaning and transformations
Listings with "minimum stay" greater than 30 often had extremely high prices, suggesting some were monthly prices. To avoid these innacuracies, we limit analysis to listings with "minimum stay" less than 7. 

Listings with high prices have no reviews at disporportionately high rates. Therefore, we remove listings with no reviews from our analysis.
```{r, echo=FALSE, warning=FALSE, fig.height=5}
raw_data <- read.csv("AB_NYC_2019.csv")
  
raw_data %>%
  mutate(quantile = ntile(price, 10)) %>%
  mutate(no_stay = number_of_reviews==0) %>% 
  group_by(quantile) %>%
  count(no_stay == 1)%>% 
  rename(Never_reviewed = `no_stay == 1`) %>% 
  mutate(prop = prop.table(n))%>% 
  ggplot(aes(x = quantile, y = prop, fill = Never_reviewed)) + geom_bar(stat = 'identity', position = 'dodge', alpha = 2/3) + geom_hline(yintercept = mean(raw_data$number_of_reviews == 0)) + ggtitle("Proportion of Never Reviewed by \n Price Decile")+ xlab("Price Decile") + ylab("Proportion")+theme(plot.title = element_text(hjust = 0.5))
```


## Unreliability of "Reviews per Month"
We can see that the study uses a different number of total months for each listing when calculating "average reviews per month."

\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & id & No.\_reviews & last\_review & Avg\_reviews \\ 
  \hline
1 & 5447434 &  20 & 2016-01-03 & 0.41 \\ 
  2 & 30232758 &  20 & 2019-07-02 & 2.84 \\ 
   \hline
\end{tabular}
\end{table}

```{r, echo=FALSE, warning=FALSE, fig.height=5}
data_transformed <- readRDS("data_transformed.rds")
before_transform <- data_transformed %>% 
  filter(id == c(5447434, 30232758)) %>% 
  dplyr::select(id, number_of_reviews, last_review, reviews_per_month)
```

However, we can recover the "months active" by using the "last review," and construct a more reliable popularity metric.

## Normalized Average Reviews
<font size ="1">
$$\frac{\text{Days between July 8, 2019 and Last Review}}{30.42} = \text{Months Inactive} $$
$$\frac{\text{No. of Reviews}}{\text{Months Active + Months Inactive}} = \text{Reviews per Month}$$
$$\text{Months Active} = \frac{\text{No. of Reviews - Avg Reviews*Months Inactive}}{\text{Reviews per Month}}$$
$$\text{Normalized Average Reviews} = \frac{\text{Total Reviews}}{\text{Months Active}}$$
</font>

## Normalized Average Reviews
Through normalization, we recover the popularity of each listing during the time it was active. 

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & id & No.\_reviews & last\_review & Nrmlized\_reviews \\ 
  \hline
1 & 5447434 &  20 & 2016-01-03 & 2.86 \\ 
  2 & 30232758 &  20 & 2019-07-02 & 2.86 \\ 
   \hline
\end{tabular}
\end{table}
```{r, echo=FALSE, warning=FALSE, fig.height=5}
after_transform <- data_transformed %>% 
  filter(id == c(5447434, 30232758)) %>% 
  dplyr::select(id, number_of_reviews, last_review, reviews_per_month, nrml_avg_reviews)
```

## Text Analysis
We identified all adjectives that occured in 10 or more listings (219 words). We then ran a linear regression of average reviews against these words, and identified all words with p-values below the Bonferonni adjustment of $\frac{0.05}{218}$. This left 18 words for further analysis.

private, square, minute, close, stock, apt, sunny, spacious, deluxe, newly, walking, central, fast, huge, west, green.
```{r, echo=FALSE, warning=FALSE, fig.height=5}
text_model <- lm(nrml_avg_reviews~ . , data = data_transformed[, -c(1:6, 7:18, 20:21)])

pvalues <- data.frame(summary(text_model)$coefficients[,4])
words <- names(coef(text_model))
toplist <- as.data.frame(cbind(words, pvalues))
colnames(toplist) <- c("words", "pvalues")

#toplist %>% 
    #arrange(pvalues) %>% 
    #filter(pvalues<0.05/219)

```

## Spatial Auto-correlation: Theater District

```{r, warning=FALSE, message=FALSE, echo= FALSE}
data <- readRDS("data_transformed.rds")
data %>% filter( neighbourhood == "Theater District" |neighbourhood == "Hell's Kitchen") %>% ggplot(aes(x  = latitude, y = longitude, color = neighbourhood)) + geom_density_2d(aes(fill = ..level..), geom= "polygon") + theme(legend.position = "none") + ggtitle("Hell's Kitchen and Theater District Density")
```

## Spatial Auto-correlation: Chinatown

```{r, warning=FALSE, message=FALSE, echo= FALSE}
data %>% filter(neighbourhood == "Chinatown") %>% 
  ggplot(aes(x  = latitude, y = longitude)) + 
  geom_density2d(aes(color = stat(level)), geom="polygon") + 
  theme(legend.position = "none") + 
  ggtitle("Chinatown Density")
```

## Conditionally Autoregressive Model
The Leroux et. al. model:

Priors:

$$\phi_k| \phi_{-k}, W, \tau^2, \rho \sim N(\frac{\rho\sum_{i=1}^k w_{ki}\phi_i}{\rho\sum_{i=1}^k w_{ki} + 1-\rho}, \frac{\tau^2}{\rho\sum_{i=1}^k w_{ki} + 1-\rho})$$
$$\tau^2 \sim Inverse-Gamma(a,b)$$
$$\rho \sim Uniform(0,1)$$

This induces neighbor spatial correlation:

$$COR(\phi_k, \phi_j | \phi_{-kj}, W, \rho) =$$
$$\frac{\rho w_{kj}}{\sqrt{(\rho \sum_{i=1}^k w_{ki} + 1 - \rho)(\rho \sum_{i=1}^k w_{ki} + 1 - \rho }}$$

## Evidence of Auto-Correlation

```{r, warning=FALSE, message=FALSE, echo= FALSE}

data_r <- rbind(c("rho", 0.9072, "Bronx"), c("rho", 0.9277, "Manhattan"),  c("rho", 0.7474, "Queens"), c("rho", 0.7389, "Brooklyn"))
data_r <- data.frame(data_r)
names(data_r) <- c("Variables", "Value", "Borough")

data_r  %>% ggplot(aes(x=Borough,y=Value)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Rho Coefficients")

```

## CAR Results Bronx

```{r, echo=FALSE, message=FALSE, warning=FALSE}

data <- readRDS("Coefficient_Data.rds")

data%>% filter(Variables != "(Intercept)" & Variables != "price" & Variables != "dist_to_subway") %>% filter(Borough == "Bronx") %>% ggplot(aes(x=Variables,y=Value)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Bronx Coefficients (Reference: Allerton)")

```


## CAR Results Manhattan

```{r, echo=FALSE, message=FALSE, warning=FALSE}
data%>% filter(Variables != "(Intercept)" & Variables != "price" & Variables != "dist_to_subway") %>% filter(Borough == "Manhattan") %>% ggplot(aes(x=Variables,y=Value)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Manhattan Coefficients (Reference: Battery Park City)")
```

## CAR Results Queens

```{r, echo=FALSE, message=FALSE, warning=FALSE}
data%>% filter(Variables != "(Intercept)" & Variables != "price" & Variables != "dist_to_subway") %>% filter(Borough == "Queens") %>% ggplot(aes(x=Variables,y=Value)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Queens Coefficients (Reference: Arverne)")
```

## CAR Results Brooklyn

```{r, echo=FALSE, message=FALSE, warning=FALSE}
data%>% filter(Variables != "(Intercept)" & Variables != "price" & Variables != "dist_to_subway") %>% filter(Borough == "Brooklyn") %>% ggplot(aes(x=Variables,y=Value)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1))+ ggtitle("Brooklyn Coefficients (Referene: Bath Beach)")
```

## Global model: shape-constrained GAM

Log-linear model
\begin{align*}
\small
  \log(&\texttt{monthly reviews}) \sim \\
  & f_1(\texttt{price}) + f_2(\texttt{dist to subway}) + f_3(\texttt{months active}) + \\
  &\texttt{entire home/apt} + \texttt{gender} + \texttt{private} + \cdots + \texttt{cozy} +\\
  &\texttt{neighbourhoods}
\end{align*}
where $f_1$, $f_2$ and $f_3$ are increasing functions.

- Allows us to estimate trends in an interpretable way.

- Tried shape-constrained generalized additive model packages (cgam and scam), but this didn't run in finite time.

- Instead boxcox transforms of the predictors.

## Results for the global model

```{r, cache=TRUE}
data <- readRDS("data_transformed.rds")
data = data %>% filter(price != 0)

model.full = lm(log(nrml_avg_reviews) ~ log(price) + log(dist_to_subway) + months_active + room_type + gender + private + square + minute + close + stock + apt + sunny + spacious + deluxe + newly + walking + cozy + central + fast + huge + west + green + neighbourhood, data=data)
```

```{r, cache=TRUE, warning=FALSE, message=FALSE}
bx = boxTidwell(log(nrml_avg_reviews) ~ price + dist_to_subway + months_active, 
           other.x= ~ room_type + gender + 
             private + square + minute + close + stock + apt + 
             sunny + spacious + deluxe + newly + walking + cozy + 
             central + fast + huge + west + green + neighbourhood, 
           data=data, max.iter=5, tol=0.01)
```

```{r, cache=TRUE}
bx_fun <- Vectorize(function(x, lambda) {
  return((x^(lambda)-1)/lambda)
})

data = data %>% mutate(whole_price = price %in% seq(25, 5000, by=25))

model.bx = lm(log(nrml_avg_reviews) ~ bx_fun(price, 0.57422) + bx_fun(dist_to_subway, 0.83087) + bx_fun(months_active, -0.27567) + entire_home + gender + whole_price + private + square + minute + close + stock + apt + sunny + spacious + deluxe + newly + walking + cozy + central + fast + huge + west + green + neighbourhood, data=data %>% mutate(entire_home= room_type == "Entire home/apt"))
```

```{r}
# Multiplicative effects as a function of quantiles
plot_coef <- function(model, name, values, col=1, add=FALSE, ...) {
  coef_est = c(coef(model)[name], confint(model)[name,])

  coef_effect <- Vectorize(function(q) {
    c(exp(coef_est[1]*quantile(values, q))/exp(coef_est[1]*quantile(values, 0.5)),
      exp(coef_est[2]*quantile(values, q))/exp(coef_est[2]*quantile(values, 0.5)),
      exp(coef_est[3]*quantile(values, q))/exp(coef_est[3]*quantile(values, 0.5)))
  })
  
  q = seq(0,0.99,length.out = 200)
  coef_curve = coef_effect(q)
  
  if(add == FALSE) {
    fun=plot
  } else {
    fun=lines
  }
  
  fun(q, coef_curve[1,], type="l", lwd=2, col=col,
      xlab="Quantile",
     ylab="Multiplicative effect compared to median",
     ...)
  polygon(c(q, rev(q)), c(coef_curve[2,], rev(coef_curve[3,])), border=NA, 
        col=adjustcolor(cmap.knitr(col), alpha.f=0.2))
}
```


```{r, fig.width=5, fig.height=4, fig.align="center"}
plot_coef(model.bx, "bx_fun(price, 0.57422)", bx_fun(data$price, 0.57422), ylim=c(0.8,1.6))
plot_coef(model.bx, "bx_fun(dist_to_subway, 0.83087)", bx_fun(data$dist_to_subway, 0.83087), 
          add=T, col=2)
plot_coef(model.bx, "bx_fun(months_active, -0.27567)", 
          bx_fun(data$months_active, -0.27567), 
          add=T, col=3)

abline(h=1, col=cmap.knitr(0), lty=2)

legend("topright", legend=c("price", "dist_to_subway", "months_active"), lty=c(1,1,1),
       col=c(cmap.knitr(1), cmap.knitr(2), cmap.knitr(3)), cex=0.8)
```

## Results for the global model

```{r, fig.width=5, fig.height=4, fig.align="center"}
keywords = c("whole_price", "entire_home", "private", "square", "minute", "close", "stock", "apt", "sunny", 
             "spacious", "deluxe", "newly", "walking", "cozy", "central", 
             "fast", "huge", "west", "green")
effects = exp(as.numeric(coef(model.bx)[paste0(keywords, "TRUE")])) %>% round(3)
confints = confint(model.bx, parm=paste0(keywords, "TRUE"))

order = order(effects)


ggplot() +
  geom_col(mapping=aes(x=reorder(keywords, -effects), y=effects-1)) + 
  geom_linerange(aes(x=reorder(keywords, -effects), ymin=confints[,1], ymax=confints[,2])) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45))+
  scale_y_continuous(labels = function(y) y + 1)+
  ylab("Multiplicative effect on popularity") +
  xlab("Keywords")
```


## Improvements

- Integrating the analyses
    - Combining keyword selection with full model fitting (e.g. with a weighted lasso).

- Non-linear relationships

